# ==========================================================
# Poker LLM Fine-tuning Configuration Guide
# ==========================================================

# [1] Base model specification
# ----------------------------------------------------------
base_model: meta-llama/Llama-3.2-1B-Instruct  

# [2] Tokenizer Configurations
# ----------------------------------------------------------
tokenizer:
  max_length: 512       

# [3] Quantization Configurations
# ----------------------------------------------------------
quant:
  load_in_4bit: True    # 4 bit quantization
  quant_type: nf4     # 4bit quantization type (nf4 is recommended)
  double_quant: True    # Double quantization for memory efficiency

# [4] LoRA Configurations
# ----------------------------------------------------------
lora:
  r: 16                  # Rank: Dimension of LoRA matrices.
  alpha: 32              # Alpha: Scaling factor for LoRA weights. (Standard convention is 2 * r)
  dropout: 0.05          # Probability of dropping neurons to prevent overfitting.
  bias: none             # Specifies if bias parameters are trained.
  task_type: CAUSAL_LM   # Task type for language modeling.
  # Target modules: Layers to apply LoRA adapters.
  target_modules:
    - q_proj             # Attention: Query
    - k_proj             # Attention: Key
    - v_proj             # Attention: Value
    - o_proj             # Attention: Output
    - gate_proj          # MLP: Gate
    - up_proj            # MLP: Up
    - down_proj          # MLP: Down

# [5] Training Configurations
# ----------------------------------------------------------
train:
  # Output & Logging
  output_dir: checkpoints/            # Directory to save checkpoints.
  report_to: none                     # Logging platform
  logging_steps: 10                   # Log training metrics (loss, LR) every K steps

  # Batch Size & Efficiency
  per_device_train_batch_size: 4      # Batch size per GPU.
  gradient_accumulation_steps: 8      # Accumulate gradients over k steps before updating weights.
                                      # Total batch size = per_device_train_batch_size * gradient_accumulation_steps
  gradient_checkpointing: True        # Trades speed for memory (Memory efficient)
  
  # Learning Rate & Scheduler
  learning_rate: 5e-5              # learning rate
  lr_scheduler_type: cosine      
  warmup_ratio: 0.05               # Ramp up LR for the first 5% of steps to stabilize training.

  # Training Duration
  max_steps: 10                    # Total training stpes (Overrides 'num_train_epochs')
  num_train_epochs: 3              

  # Precision & Optimizer
  bf16: True                       # Set True for A100, H100
  fp16: False                      # Otherwise
  optim: paged_adamw_8bit          # 8-bit optimizer. (Crucial for QLoRA to save memory)
                                   # 'paged' handles memory spikes to prevent OOM crashes.

  # Supervised Fine-Tuning Configurations
  completion_only_loss: True          # Only calculates loss on the 'Answer', ignoring the prompt.

  # Evaluation & Saving Strategy
  eval_strategy: steps      # Evaluate every 'eval_steps'. Can also be 'epoch'.
  eval_steps: 5             # Run evaluation every 5 steps.
  save_strategy: steps      # Save checkpoint every 'save_steps'. Must match eval_strategy.
  save_steps: 5             # Save checkpoint every 5 steps.
  save_total_limit: 2       # Keep only the last 2 checkpoints to save disk space.

  # Best Model Selection
  load_best_model_at_end: True    # After training, load the checkpoint with the best metric.
  metric_for_best_model: eval_loss # Metric to monitor.
  greater_is_better: False        # False because lower Loss is better.